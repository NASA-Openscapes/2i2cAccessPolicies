---
title: "NASA Openscapes 2i2c JupyterHub\nUsage and Costs"
params:
  year_month: "2024-05"
subtitle: "Monthly report for `r format(lubridate::ym(params$year_month), '%B %Y')`"
format: pdf
---

<!-- 
To render this using the quarto cli use:
ym="2024-06" # set the year and month parameter
quarto render aws-usage-report/aws-usage-report.qmd -P year_month:$ym --output "aws-usage-report_$ym.pdf"
 -->

### Introduction

A key objective of NASA Openscapes is to minimize “the time to science” for researchers. Cloud infrastructure can facilitate shortening this time. We use a 2i2c-managed JupyterHub ("Hub"), which lets us work in the cloud next to NASA Earthdata in AWS US-West-2. The purpose of the JupyterHub is to provide initial, exploratory experiences accessing NASA Earthdata in the cloud. It is not meant to be a long-term solution to support on-going science work or software development. For those users that decide working in the Cloud is advantageous and want to move there, we support a migration from the Hub to their own environment through Coiled.io, and are working on other "fledging" pathways.

The main costs of running the JupyterHub come from two sources:

1. Compute, using AWS EC2
2. Storage using AWS EFS, via storage in users' home directories

Compute costs scale up and down as the Hub is used, however storage costs are 
fixed - we pay for "data at rest", with ongoing daily costs/GB even while the Hub is
not running.

Storing large amounts of data in the cloud can incur significant ongoing costs if not done optimally. We are developing [technical strategies and policies](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/policies-admin/data-policies.html) to reduce storage costs that will keep the Openscapes 2i2c Hub a shared resource for us all to use, while also providing reusable strategies for other admins.

This report is intended to give a monthly summary of usage of the Hub and its 
resources, by tracking metrics on costs and usage of key components of storage (EFS)
and compute (EC2).

```{r setup}
#| include: false

library(dplyr)
library(kyber)
library(ggplot2)
library(forcats)
library(lubridate)
library(paws)
library(here)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

source(here("aws-usage-report/R/prometheus-utils.R"))
source(here("aws-usage-report/R/aws-ce-utils.R"))

if (interactive()) {
  params <- list(year_month = format(Sys.Date() %m-% months(1), "%Y-%m"))
}

start_date <- ym(params$year_month)
end_date <- ceiling_date(start_date, unit = "month") - days(1)

cost_explorer <- paws::costexplorer()

theme_set(theme_classic())
```

## Month over month changes

### Total Costs

```{r total-costs}

# https://www.paws-r-sdk.com/docs/costexplorer_get_cost_and_usage/
# https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/API_GetDimensionValues.html
total_monthly_usage_costs <- cost_explorer$get_cost_and_usage(
  TimePeriod = list(Start = ceiling_date(end_date %m-% months(6), unit = "month"), End = end_date),
  Granularity = "MONTHLY",
  Filter = list(Dimensions = list(
    Key = "RECORD_TYPE",
    Values = "Usage"
  )),
  Metrics = "UnblendedCost"
) |>
  ce_to_df()

ggplot(total_monthly_usage_costs) +
  geom_line(aes(x = start_date, y = UnblendedCost)) +
  labs(
    title = "Monthly total cost of all AWS Services for running the\nNASA Openscapes 2i2c Hub",
    x = "Month",
    y = "Monthly cost ($)"
  )
```

```{r monthly-costs-by-service}
#| fig-height: 8

monthly_costs_by_service <- cost_explorer$get_cost_and_usage(
  TimePeriod = list(Start = ceiling_date(end_date %m-% months(6), unit = "month"), End = end_date),
  Granularity = "MONTHLY",
  Filter = list(Dimensions = list(
    Key = "RECORD_TYPE",
    Values = "Usage"
  )),
  Metrics = "UnblendedCost",
  GroupBy = list(
    list(
      Type = "DIMENSION",
      Key = "SERVICE"
    )
  )
) |>
  ce_to_df()

monthly_costs_by_service |>
  group_by(start_date, end_date) |>
  mutate(service = case_when(
    UnblendedCost < 1 ~ "Other",
    .default = service
  )) |>
  group_by(start_date, end_date, service) |>
  summarise(
    UnblendedCost = sum(UnblendedCost, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    service = fct_reorder(service, desc(UnblendedCost), .fun = mean, .desc = TRUE)
  ) |>
  ggplot(aes(x = start_date, y = UnblendedCost, fill = service)) +
  geom_col() +
  paletteer::scale_fill_paletteer_d("ggpomological::pomological_palette", direction = -1) +
  guides(fill = guide_legend(ncol = 2)) +
  theme(legend.position = "bottom", legend.title.position = "top") +
  labs(
    title = "Monthly cost of AWS Services",
    caption = "*Services costing less than $1/month are grouped into 'Other'",
    x = "Month",
    y = "Monthly cost ($)",
    fill = "AWS Service"
  )

```

### Storage

```{r}
monthly_size <- query_prometheus_range(
  query = "max(dirsize_total_size_bytes{namespace='prod'})",
  start_time = end_date %m-% months(6),
  end_time = end_date,
  step = 60 * 60 * 24 * 30
) |>
  create_range_df(value_name = "size")

ggplot(monthly_size) +
  geom_col(aes(x = date, y = size)) +
  labs(
    title = "Total size of user home directories in AWS EFS\nin the main Hub",
    x = "Month",
    y = "Total Size (GB)"
  )
```

## Detailed breakdown for the month of `r format(start_date, "%B")`

### Home directory sizes

```{r homedir-size-by-date}
size_by_date <- query_prometheus_range(
  query = "max(dirsize_total_size_bytes) by (directory, namespace)",
  start_time = start_date,
  end_time = end_date,
  step = 60 * 60 * 24
) |>
  create_range_df(value_name = "size") |>
  mutate(
    directory = unsanitize_dir_names(directory)
  )

# list_teams("nasa-openscapes")
# list_teams("nasa-openscapes-workshops")

lt_access_members <- list_team_members(
  team = "LongtermAccess-2i2c",
  org = "nasa-openscapes",
  names_only = TRUE
) |>
  tolower()

champions_members <- list_team_members(
  team = "nasa-champions-2024",
  org = "nasa-openscapes-workshops",
  names_only = TRUE
) |>
  tolower() |>
  setdiff(lt_access_members)

teams <- data.frame(
  team = "NASA Champions 2024",
  user = champions_members
) |>
  bind_rows(
    data.frame(
      team = "Long Term Access",
      user = lt_access_members
    )
  )

# setdiff(champions_members, unique(size_by_date$directory))

size_by_date <- size_by_date |>
  left_join(
    teams,
    by = join_by(directory == user)
  ) |>
  mutate(
    team = ifelse(namespace == "workshop", "N/A", team),
    directory = fct_reorder(directory, desc(size), .fun = max, .desc = TRUE)
  )

all_dirs_sum_by_date <- size_by_date |>
  filter(namespace %in% c("prod", "workshop")) |>
  group_by(namespace, date, team) |>
  summarize(total_size_gb = sum(size)) |>
  mutate(
    team = ifelse(is.na(team) & namespace == "prod", "Other", team),
    team = fct_reorder(team, desc(total_size_gb), .fun = max, .desc = TRUE)
  )
```

```{r homedir-size-over-time}
all_dirs_sum_by_date |>
  ggplot(aes(x = date, y = total_size_gb)) +
  geom_area(aes(fill = team)) +
  facet_grid(vars(namespace), scales = "free_y") +
  theme(legend.position = "bottom", legend.title.position = "top") +
  paletteer::scale_fill_paletteer_d("ggpomological::pomological_palette") +
  labs(
    title = "Total size of user home directories by access team\nand Hub namespace",
    x = "Date",
    y = "Size (GiB)",
  )
```

```{r homedir-size-champions}
size_by_date |>
  filter(team == "NASA Champions 2024") |>
  ggplot(aes(x = date, y = size, fill = directory)) +
  geom_area() +
  paletteer::scale_fill_paletteer_d("khroma::soil", guide = "none") +
  labs(
    title = "Size of home directories by user for 2024 Champions cohort",
    x = "Date",
    y = "Size (GiB)"
  )

```

### Compute costs and usage

```{r ec2-costs}
# https://www.paws-r-sdk.com/docs/costexplorer_get_cost_and_usage/
# https://docs.aws.amazon.com/aws-cost-management/latest/APIReference/API_GetDimensionValues.html

# TODO: modify ce_to_df to deal with an arbitrary number of metrics so
# we can do this in one call with `Metrics = list("UnblendedCost", "UsageQuantity")
# and pass it to ce_to_df() once, rather than joining
ec2_instance_type_costs_usage_res <- cost_explorer$get_cost_and_usage(
  TimePeriod = list(Start = start_date, End = end_date),
  Granularity = "DAILY",
  Filter = list(
    Dimensions = list(
      Key = "RECORD_TYPE",
      Values = "Usage"
    ),
    Dimensions = list(
      Key = "SERVICE",
      Values = "Amazon Elastic Compute Cloud - Compute"
    )
  ),
  Metrics = list("UnblendedCost", "UsageQuantity"),
  GroupBy = list(
    list(
      Type = "DIMENSION",
      Key = "SERVICE"
    ),
    list(
      Type = "DIMENSION",
      Key = "INSTANCE_TYPE"
    )
  )
)

# Join costs and usage hours
ec2_instance_type_costs_usage <- ec2_instance_type_costs_usage_res |>
  ce_to_df(metric = "UnblendedCost") |>
  left_join(
    ce_to_df(ec2_instance_type_costs_usage_res, metric = "UsageQuantity"),
    by = c("start_date", "end_date", "service", "instance_type")
  ) |>
  filter(
    instance_type != "NoInstanceType"
  )

instances <- unique(ec2_instance_type_costs_usage$instance_type)

instance_palette <- paletteer::paletteer_d(
  "ggpomological::pomological_palette",
  n = length(instances)
) |>
  setNames(instances)
```

```{r daily-usage-by-instance}
ec2_instance_type_costs_usage |>
  mutate(instance_type = fct_reorder(instance_type, desc(UsageQuantity), .fun = max, .desc = TRUE)) |>
  ggplot(aes(x = start_date, y = UsageQuantity, fill = instance_type)) +
  geom_col() +
  scale_fill_manual(values = instance_palette) +
  labs(
    title = "Daily EC2 usage by instance type",
    x = "Date",
    y = "Usage (hours)",
    fill = "EC2 Instance Type"
  )
```

```{r daily-cost-by-instance}
ec2_instance_type_costs_usage |>
  mutate(instance_type = fct_reorder(instance_type, desc(UnblendedCost), .fun = max, .desc = TRUE)) |>
  ggplot(aes(x = start_date, y = UnblendedCost, fill = instance_type)) +
  geom_col() +
  scale_fill_manual(values = instance_palette) +
  labs(
    title = "Daily EC2 cost by instance type",
    x = "Date",
    y = "Daily Cost ($)",
    fill = "EC2 Instance Type"
  )
```

```{r total-usage-vs-cost-by-instance}
ec2_instance_type_costs_usage |>
  group_by(instance_type) |>
  summarize(
    total_hours = sum(UsageQuantity),
    total_cost = sum(UnblendedCost)
  ) |>
  ggplot(aes(x = total_hours, y = total_cost, colour = instance_type)) +
  geom_point(size = 5) +
  scale_colour_manual(values = instance_palette) +
  labs(
    title = "Total cost vs hours on different EC2 instance types",
    x = "Hours",
    y = "EC2 Instance Type"
  )
```
